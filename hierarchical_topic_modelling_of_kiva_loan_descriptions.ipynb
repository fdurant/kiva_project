{
 "metadata": {
  "name": "",
  "signature": "sha256:88f823a4e93bdc9c9f7891c6f0d4f36970da0c827b35d166c259f2263fa9d022"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Hierarchical topic modelling of Kiva loan descriptions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Goals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*  Create a visual explorer for a static set of 800K+ English loan descriptions from Kiva.org\n",
      "*  Use (hierarchical) topic modelling\n",
      "*  Publish the explorer to the world"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* David Mimno and Andrew MacCallum: [Organizing the OCA: Learning Faceted Subjects from a Library of Digital Books](http://mimno.infosci.cornell.edu/papers/f129-mimno.pdf) => bottom-up approach\n",
      "* Jay Pujara and Peter Skomoroch: [Large-Scale Hierarchical Topic Models](http://linqs.cs.umd.edu/basilic/web/Publications/2012/pujara:nips12/pujara_biglearn12.pdf) => top-down approach\n",
      "* Alison Smith, Timothy Hawes, and Meredith Myers: [Hi&eacute;rarchie: Interactive Visualization for Hierarchical Topic Models](http://nlp.stanford.edu/events/illvi2014/papers/smith-illvi2014b.pdf)\n",
      "* Hierarchie [Readme page at GitHub](http://mlvl.github.io/Hierarchie/#/about)\n",
      "* [Gensim - topic modelling for humans](https://radimrehurek.com/gensim/index.html)\n",
      "* Nikolaos Aletras and Mark Stevenson: [Measuring the Similarity between Automatically Generated Topics](http://staffwww.dcs.shef.ac.uk/people/N.Aletras/resources/2014_eacl_topicSim_short.pdf)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Use gensim to derive flat topic models over (part of) the Kiva corpus, taking the [tutorial](https://radimrehurek.com/gensim/tutorial.html) as guideline\n",
      "* Organize the found topic models into a hierarchy - no precise idea yet on how to do this (maybe some form of hierarchical clustering may apply)\n",
      "* Convert that hierarchy into a [JSON data file](https://github.com/mlvl/Hierarchie/tree/gh-pages/app/data) compliant with Hierarchie\n",
      "* Visualize everything with Hierarchie\n",
      "\n",
      "IMPORTANT: store python code in scripts, and call these from ipython notebook"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data preprocessing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 1: download [Kiva data dump (JSON format)](http://s3.kiva.org/snapshots/kiva_ds_json.zip), and extract into data/static"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 2: since the 'description' fields in the Kiva data dump often mix multiple languages (due to manual translations), the language codes are not reliable. Therefore, we:\n",
      "\n",
      "* split the descriptions in paragraps\n",
      "* do language detection on the paragraphs\n",
      "* store the recombined paragraphs and their language code in new processed_description field\n",
      "\n",
      "The data are written to a locally installed MongoDB 'kiva', collection 'loans'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Next line commented out because we only want to run this once\n",
      "# nohup python src/load_kiva_to_mongodb.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 3: load (a subset) of the English data from MongoDB, and convert it to the [Blei LDA-C format](http://www.cs.princeton.edu/~blei/lda-c/), using a [gensim utility function](https://radimrehurek.com/gensim/tut1.html#corpus-formats)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python src/convert_mongodb_to_blei_ldac.py --dataDir data/topicmodelling\\\n",
      "                                            --corpusBaseName kiva \\\n",
      "                                            --stopwordFile=data/topicmodelling/kiva_stopwords.tsv \\\n",
      "                                            --maxNrDocs 250000 \\\n",
      "                                            --filterBelow 10 \\\n",
      "                                            --filterAbove 0.5 \\\n",
      "                                            --filterKeepN 1000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating MongoDB cursor ... done\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of loans in 'en' since 2014: 170676\r\n",
        "First pass: streaming from MongoDB ...\r\n",
        "creating the dictionary ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 5000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 10000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 15000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 20000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 25000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 30000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 35000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 40000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 45000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 50000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 55000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 60000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 65000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 70000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 75000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 80000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 85000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 90000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 95000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 100000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 105000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 110000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 115000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 120000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 125000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 130000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 135000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 140000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 145000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 150000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 155000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 160000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 165000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 170000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "filtering the dictionary ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " done\r\n",
        "wrote data/topicmodelling/kiva_dict.bin ... and data/topicmodelling/kiva_dict.txt ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " done\r\n",
        "Second pass: streaming from MongoDB ... saving into data/topicmodelling/kiva.lda-c (Blei corpus format) ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " done\r\n",
        "Number of documents converted: 170676\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Vocabulary size: 1000\r\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python src/model_topics.py --dataDir data/topicmodelling \\\n",
      "                            --modelDir data/topicmodelling \\\n",
      "                            --corpusBaseName kiva \\\n",
      "                            --nrTopics 64 \\\n",
      "                            --nrWords 8"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading Blei corpus file data/topicmodelling/kiva.lda-c ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " done\r\n",
        "<gensim.corpora.bleicorpus.BleiCorpus object at 0x108480ad0>\r\n",
        "Dictionary(1000 unique tokens: [u'neighbors', u'limited', u'managed', u'lack', u'livelihood']...)\r\n",
        "Making topic model ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " done\r\n",
        "0.030*good + 0.027*cater + 0.026*husband + 0.026*income + 0.026*woman + 0.026*pkr + 0.025*married + 0.020*purchase\r\n",
        "0.193*vegetables + 0.129*food + 0.079*meat + 0.065*fruits + 0.058*chicken + 0.050*restaurant + 0.043*ingredients + 0.040*stall\r\n",
        "0.227*store + 0.074*grocery + 0.067*products + 0.044*small + 0.036*basic + 0.024*drinks + 0.023*grains + 0.020*items\r\n",
        "0.068*month + 0.061*mobile + 0.052*lives + 0.040*earns + 0.036*type + 0.032*metal + 0.029*stall + 0.029*requested\r\n",
        "0.046*house + 0.037*enough + 0.034*applied + 0.032*jos\u00e9 + 0.031*living + 0.030*money + 0.027*order + 0.027*purchase\r\n",
        "0.078*farming + 0.044*dairy + 0.042*poultry + 0.039*kenya + 0.025*farmers + 0.025*income + 0.024*area + 0.022*farm\r\n",
        "0.091*credit + 0.076*access + 0.064*provides + 0.048*asks + 0.036*loans + 0.034*products + 0.034*times + 0.033*new\r\n",
        "0.143*would + 0.139*like + 0.045*school + 0.043*old + 0.041*married + 0.029*food + 0.024*future + 0.022*sell\r\n",
        "0.201*group + 0.079*members + 0.039*one + 0.035*women + 0.027*hand + 0.024*part + 0.021*program + 0.020*poverty\r\n",
        "0.100*pay + 0.064*university + 0.056*fees + 0.039*studies + 0.039*job + 0.035*tuition + 0.034*year + 0.033*medical\r\n",
        "0.039*fellowship + 0.038*community + 0.036*god + 0.029*groups + 0.028*week + 0.027*word + 0.023*php + 0.022*partners\r\n",
        "0.050*maria + 0.041*income + 0.034*asking + 0.033*help + 0.031*woman + 0.030*improve + 0.027*usd + 0.027*taking\r\n",
        "0.081*land + 0.044*crops + 0.044*corn + 0.035*agriculture + 0.034*agricultural + 0.025*beans + 0.022*fertilizers + 0.021*crop\r\n",
        "0.054*get + 0.052*day + 0.039*every + 0.038*able + 0.035*ahead + 0.032*work + 0.030*life + 0.023*good\r\n",
        "0.065*hours + 0.059*day + 0.048*says + 0.042*! + 0.038*wanted + 0.032*change + 0.026*plans + 0.025*received\r\n",
        "0.126*school + 0.053*education + 0.039*husband + 0.031*daughter + 0.026*students + 0.024*married + 0.024*one + 0.023*two\r\n",
        "0.116*sugar + 0.112*rice + 0.102*oil + 0.098*flour + 0.080*cooking + 0.065*shop + 0.047*bread + 0.038*beans\r\n",
        "0.135*fish + 0.034*php + 0.033*good + 0.032*pmpc + 0.029*capital + 0.029*hopes + 0.028*income + 0.027*cooperative\r\n",
        "0.180*babban + 0.129*gona + 0.106*farmer + 0.084*acre + 0.079*member + 0.060*asked + 0.050*better + 0.046*use\r\n",
        "0.029*work + 0.028*able + 0.026*parents + 0.025*help + 0.021*young + 0.016*professional + 0.016*support + 0.015*wants\r\n",
        "0.071*husband + 0.042*village + 0.037*son + 0.033*lives + 0.029*help + 0.026*taxi + 0.025*income + 0.025*engaged\r\n",
        "0.181*borrower + 0.135*2013 + 0.125*financing + 0.121*active + 0.102*enterprise + 0.023*november + 0.021*since + 0.021*help\r\n",
        "0.112*sell + 0.096*low + 0.087*prices + 0.079*price + 0.079*higher + 0.079*months + 0.076*market + 0.062*needs\r\n",
        "0.071*home + 0.042*old + 0.038*house + 0.028*living + 0.027*lives + 0.024*soon + 0.022*wife + 0.019*works\r\n",
        "0.047*electricity + 0.041*school + 0.033*charcoal + 0.031*selling + 0.029*boost + 0.029*piped + 0.029*house + 0.028*operated\r\n",
        "0.147*farm + 0.116*farming + 0.060*fertilizer + 0.048*rice + 0.045*seeds + 0.044*fertilizers + 0.035*inputs + 0.032*manure\r\n",
        "0.074*start + 0.073*financial + 0.041*activity + 0.037*lenders + 0.032*income + 0.032*support + 0.028*stable + 0.026*program\r\n",
        "0.030*woman + 0.023*able + 0.022*selling + 0.020*products + 0.019*mar\u00eda + 0.018*improve + 0.017*order + 0.017*work\r\n",
        "0.063*nwtf + 0.044*philippines + 0.043*php + 0.037*save + 0.032*money + 0.032*hard + 0.032*married + 0.031*enough\r\n",
        "0.039*construction + 0.039*since + 0.038*work + 0.036*using + 0.035*light + 0.034*fuel + 0.027*reduce + 0.026*workshop\r\n",
        "0.068*fund + 0.048*total + 0.044*hoa + 0.039*thanh + 0.033*group + 0.030*poor + 0.030*one + 0.029*district\r\n",
        "0.146*previous + 0.105*successfully + 0.064*another + 0.057*repaid + 0.057*paid + 0.055*loans + 0.054*back + 0.049*new\r\n",
        "0.067*customers + 0.057*increase + 0.038*income + 0.030*hopes + 0.029*improve + 0.029*needs + 0.027*products + 0.025*living\r\n",
        "0.064*profit + 0.040*back + 0.037*used + 0.034*school + 0.032*paying + 0.028*make + 0.027*daily + 0.024*able\r\n",
        "0.134*personal + 0.061*spare + 0.060*parts + 0.041*previously + 0.040*children\u2019s + 0.037*help + 0.036*big + 0.036*piece\r\n",
        "0.064*clothing + 0.038*selling + 0.026*sells + 0.025*sales + 0.024*shoes + 0.023*products + 0.022*likes + 0.020*city\r\n",
        "0.193*clothes + 0.143*fishing + 0.123*sewing + 0.089*machine + 0.055*materials + 0.047*tailoring + 0.040*cloth + 0.039*fabric\r\n",
        "0.065*use + 0.045*blessed + 0.036*income + 0.034*profits + 0.032*married + 0.031*hopes + 0.030*old + 0.027*future\r\n",
        "0.045*lives + 0.038*better + 0.034*life + 0.033*income + 0.026*quality + 0.024*household + 0.023*home + 0.023*able\r\n",
        "0.287*maize + 0.267*harvest + 0.151*season + 0.047*able + 0.035*purchasing + 0.027*seed + 0.026*fertilizer + 0.022*time\r\n",
        "0.152*bank + 0.073*communal + 0.040*member + 0.038*dream + 0.031*members + 0.030*selling + 0.029*happy + 0.029*requesting\r\n",
        "0.207*water + 0.054*village + 0.052*solar + 0.050*save + 0.048*safe + 0.042*access + 0.035*clean + 0.030*money\r\n",
        "0.036*work + 0.032*lives + 0.031*animals + 0.031*district + 0.025*agriculture + 0.025*area + 0.024*works + 0.024*old\r\n",
        "0.115*time + 0.089*later + 0.057*continues + 0.054*excited + 0.053*reliable + 0.049*promises + 0.040*member + 0.037*payments\r\n",
        "0.193*milk + 0.069*cash + 0.065*cow + 0.050*immediate + 0.047*production + 0.044*clothing + 0.034*cheese + 0.033*cows\r\n",
        "0.055*describes + 0.044*use + 0.041*operates + 0.037*involved + 0.036*biggest + 0.035*microfinance + 0.035*goal + 0.035*customers\r\n",
        "0.082*store + 0.071*general + 0.048*goods + 0.045*items + 0.041*like + 0.040*runs + 0.034*sell + 0.034*provide\r\n",
        "0.057*market + 0.052*husband + 0.051*fruit + 0.047*sells + 0.043*two + 0.039*daughters + 0.037*selling + 0.032*married\r\n",
        "0.151*pigs + 0.106*raising + 0.094*feed + 0.052*chickens + 0.050*lenders + 0.032*raise + 0.029*support + 0.027*grateful\r\n",
        "0.046*include + 0.043*school + 0.042*major + 0.033*retail + 0.029*days + 0.028*lives + 0.026*house + 0.026*uganda\r\n",
        "0.095*living + 0.055*improve + 0.055*pig + 0.048*fattening + 0.041*income + 0.033*future + 0.032*conditions + 0.031*feeds\r\n",
        "0.059*livestock + 0.048*cattle + 0.046*cows + 0.034*income + 0.032*sheep + 0.031*kgs + 0.029*animal + 0.023*farm\r\n",
        "0.240*community + 0.054*became + 0.044*use + 0.035*joined + 0.031*bananas + 0.030*needs + 0.028*small + 0.026*member\r\n",
        "0.070*town + 0.057*manage + 0.057*help + 0.052*finances + 0.050*expand + 0.049*bags + 0.046*able + 0.042*past\r\n",
        "0.059*drinking + 0.040*produce + 0.039*income + 0.039*man + 0.037*living + 0.029*sell + 0.027*married + 0.027*natural\r\n",
        "0.133*coffee + 0.064*efforts + 0.059*dreams + 0.053*place + 0.043*help + 0.038*hardworking + 0.038*end + 0.035*comfortable\r\n",
        "0.060*husband + 0.056*pakistan + 0.053*partner + 0.045*field + 0.038*brac + 0.035*therefore + 0.027*needs + 0.026*due\r\n",
        "0.070*care + 0.042*takes + 0.040*also + 0.036*even + 0.030*one + 0.029*nearby + 0.028*wife + 0.027*two\r\n",
        "0.119*services + 0.074*service + 0.062*transport + 0.059*motorcycle + 0.045*maintenance + 0.045*transportation + 0.044*vehicle + 0.040*tools\r\n",
        "0.050*women + 0.039*average + 0.035*group + 0.021*live + 0.019*profit + 0.018*use + 0.018*francs + 0.016*plans\r\n",
        "0.131*house + 0.110*build + 0.067*cement + 0.055*materials + 0.048*building + 0.046*sand + 0.033*lives + 0.031*healthy\r\n",
        "0.111*believes + 0.105*plant + 0.074*acres + 0.064*despite + 0.055*current + 0.050*although + 0.047*loves + 0.040*educated\r\n",
        "0.050*rice + 0.045*province + 0.042*living + 0.035*group + 0.029*income + 0.027*day + 0.026*cambodia + 0.023*support\r\n",
        "0.086*institution + 0.063*[ + 0.063*] + 0.060*de + 0.055*customer + 0.049*traditional + 0.048*la + 0.047*planning\r\n",
        "Writing model file data/topicmodelling/kiva.lda_model ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " done\r\n",
        "Creating complete topic/word matrix in memory:\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 1/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 2/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 3/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 4/64 ...\r\n",
        "topic 5/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 6/64 ...\r\n",
        "topic 7/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 8/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 9/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 10/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 11/64 ...\r\n",
        "topic 12/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 13/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 14/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 15/64 ...\r\n",
        "topic 16/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 17/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 18/64 ...\r\n",
        "topic 19/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 20/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 21/64 ...\r\n",
        "topic 22/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 23/64 ...\r\n",
        "topic 24/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 25/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 26/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 27/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 28/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 29/64 ...\r\n",
        "topic 30/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 31/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 32/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 33/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 34/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 35/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 36/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 37/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 38/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 39/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 40/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 41/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 42/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 43/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 44/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 45/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 46/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 47/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 48/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 49/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 50/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 51/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 52/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 53/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 54/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 55/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 56/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 57/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 58/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 59/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 60/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 61/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 62/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 63/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 64/64 ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done\r\n",
        "Writing topic/word matrix file data/topicmodelling/kiva_topic_words_matrix.h5 ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/local/lib/python2.7/site-packages/pandas/io/pytables.py:2486: PerformanceWarning: \r\n",
        "your performance may suffer as PyTables will pickle object types that it cannot\r\n",
        "map directly to c-types [inferred_type->unicode,key->axis0] [items->None]\r\n",
        "\r\n",
        "  warnings.warn(ws, PerformanceWarning)\r\n",
        "/usr/local/lib/python2.7/site-packages/pandas/io/pytables.py:2486: PerformanceWarning: \r\n",
        "your performance may suffer as PyTables will pickle object types that it cannot\r\n",
        "map directly to c-types [inferred_type->unicode,key->block0_items] [items->None]\r\n",
        "\r\n",
        "  warnings.warn(ws, PerformanceWarning)\r\n",
        "done\r\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python src/infer_document_topic_distributions.py --modelDir data/topicmodelling \\\n",
      "                                                  --modelBaseName kiva \\\n",
      "                                                  --maxNrDocs 250000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading topic/word matrix file data/topicmodelling/kiva_topic_words_matrix.h5 ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " done\r\n",
        "Loading model file data/topicmodelling/kiva.lda_model ... done\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading Blei corpus file data/topicmodelling/kiva.lda-c ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " done\r\n",
        "<gensim.corpora.bleicorpus.BleiCorpus object at 0x1064467d0>\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 5000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 10000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 15000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 20000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 25000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 30000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 35000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 40000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 45000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 50000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 55000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 60000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 65000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 70000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 75000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 80000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 85000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 90000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 95000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 100000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 105000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 110000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 115000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 120000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 125000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 130000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 135000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 140000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 145000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 150000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 155000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 160000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 165000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 170000 documents ...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3373 winning (1.98%); 1.82% weight in good cater husband income woman pkr married purchase\r\n",
        "168 winning (0.10%); 0.94% weight in vegetables food meat fruits chicken restaurant ingredients stall\r\n",
        "1502 winning (0.88%); 1.38% weight in store grocery products small basic drinks grains items\r\n",
        "1905 winning (1.12%); 0.89% weight in month mobile lives earns type metal stall requested\r\n",
        "2418 winning (1.42%); 1.41% weight in house enough applied jos\u00e9 living money order purchase\r\n",
        "2786 winning (1.63%); 1.68% weight in farming dairy poultry kenya farmers income area farm\r\n",
        "631 winning (0.37%); 0.83% weight in credit access provides asks loans products times new\r\n",
        "2884 winning (1.69%); 1.77% weight in would like school old married food future sell\r\n",
        "3109 winning (1.82%); 2.01% weight in group members one women hand part program poverty\r\n",
        "2649 winning (1.55%); 1.35% weight in pay university fees studies job tuition year medical\r\n",
        "2560 winning (1.50%); 2.55% weight in fellowship community god groups week word php partners\r\n",
        "3348 winning (1.96%); 1.49% weight in maria income asking help woman improve usd taking\r\n",
        "3342 winning (1.96%); 2.08% weight in land crops corn agriculture agricultural beans fertilizers crop\r\n",
        "4658 winning (2.73%); 2.88% weight in get day every able ahead work life good\r\n",
        "729 winning (0.43%); 0.85% weight in hours day says ! wanted change plans received\r\n",
        "3779 winning (2.21%); 2.46% weight in school education husband daughter students married one two\r\n",
        "388 winning (0.23%); 1.03% weight in sugar rice oil flour cooking shop bread beans\r\n",
        "2577 winning (1.51%); 1.08% weight in fish php good pmpc capital hopes income cooperative\r\n",
        "2469 winning (1.45%); 0.93% weight in babban gona farmer acre member asked better use\r\n",
        "9259 winning (5.42%); 3.90% weight in work able parents help young professional support wants\r\n",
        "3390 winning (1.99%); 1.90% weight in husband village son lives help taxi income engaged\r\n",
        "292 winning (0.17%); 0.43% weight in borrower 2013 financing active enterprise november since help\r\n",
        "643 winning (0.38%); 0.74% weight in sell low prices price higher months market needs\r\n",
        "6002 winning (3.52%); 2.46% weight in home old house living lives soon wife works\r\n",
        "3196 winning (1.87%); 1.61% weight in electricity school charcoal selling boost piped house operated\r\n",
        "901 winning (0.53%); 1.30% weight in farm farming fertilizer rice seeds fertilizers inputs manure\r\n",
        "926 winning (0.54%); 1.22% weight in start financial activity lenders income support stable program\r\n",
        "6144 winning (3.60%); 2.80% weight in woman able selling products mar\u00eda improve order work\r\n",
        "21206 winning (12.42%); 7.23% weight in nwtf philippines php save money hard married enough\r\n",
        "861 winning (0.50%); 1.24% weight in construction since work using light fuel reduce workshop\r\n",
        "2888 winning (1.69%); 1.47% weight in fund total hoa thanh group poor one district\r\n",
        "58 winning (0.03%); 0.78% weight in previous successfully another repaid paid loans back new\r\n",
        "4648 winning (2.72%); 2.42% weight in customers increase income hopes improve needs products living\r\n",
        "1428 winning (0.84%); 1.22% weight in profit back used school paying make daily able\r\n",
        "180 winning (0.11%); 0.74% weight in personal spare parts previously children\u2019s help big piece\r\n",
        "4899 winning (2.87%); 2.46% weight in clothing selling sells sales shoes products likes city\r\n",
        "201 winning (0.12%); 0.75% weight in clothes fishing sewing machine materials tailoring cloth fabric\r\n",
        "3255 winning (1.91%); 1.45% weight in use blessed income profits married hopes old future\r\n",
        "6113 winning (3.58%); 2.67% weight in lives better life income quality household home able\r\n",
        "11 winning (0.01%); 0.67% weight in maize harvest season able purchasing seed fertilizer time\r\n",
        "2240 winning (1.31%); 0.94% weight in bank communal member dream members selling happy requesting\r\n",
        "1862 winning (1.09%); 1.15% weight in water village solar save safe access clean money\r\n",
        "3937 winning (2.31%); 1.96% weight in work lives animals district agriculture area works old\r\n",
        "133 winning (0.08%); 0.69% weight in time later continues excited reliable promises member payments\r\n",
        "239 winning (0.14%); 0.73% weight in milk cash cow immediate production clothing cheese cows\r\n",
        "3184 winning (1.87%); 1.42% weight in describes use operates involved biggest microfinance goal customers\r\n",
        "5172 winning (3.03%); 2.76% weight in store general goods items like runs sell provide\r\n",
        "2157 winning (1.26%); 1.46% weight in market husband fruit sells two daughters selling married\r\n",
        "753 winning (0.44%); 1.02% weight in pigs raising feed chickens lenders raise support grateful\r\n",
        "1229 winning (0.72%); 0.99% weight in include school major retail days lives house uganda\r\n",
        "2962 winning (1.74%); 1.41% weight in living improve pig fattening income future conditions feeds\r\n",
        "3194 winning (1.87%); 1.67% weight in livestock cattle cows income sheep kgs animal farm\r\n",
        "2222 winning (1.30%); 1.09% weight in community became use joined bananas needs small member\r\n",
        "1490 winning (0.87%); 1.04% weight in town manage help finances expand bags able past\r\n",
        "2969 winning (1.74%); 1.17% weight in drinking produce income man living sell married natural\r\n",
        "332 winning (0.19%); 0.82% weight in coffee efforts dreams place help hardworking end comfortable\r\n",
        "3271 winning (1.92%); 1.79% weight in husband pakistan partner field brac therefore needs due\r\n",
        "460 winning (0.27%); 1.09% weight in care takes also even one nearby wife two\r\n",
        "473 winning (0.28%); 0.93% weight in services service transport motorcycle maintenance transportation vehicle tools\r\n",
        "3900 winning (2.29%); 2.37% weight in women average group live profit use francs plans\r\n",
        "1311 winning (0.77%); 1.22% weight in house build cement materials building sand lives healthy\r\n",
        "24 winning (0.01%); 0.47% weight in believes plant acres despite current although loves educated\r\n",
        "5320 winning (3.12%); 2.33% weight in rice province living group income day cambodia support\r\n",
        "66 winning (0.04%); 0.62% weight in institution [ ] de customer traditional la planning\r\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python src/build_topic_hierarchy.py --modelDir data/topicmodelling \\\n",
      "                                     --modelBaseName kiva \\\n",
      "                                     --nrClusters 16 \\\n",
      "                                     --nrWords 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading topic/word matrix file data/topicmodelling/kiva_topic_words_matrix.h5 ... done\r\n",
        "Hierarchically clustering topics ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "recursive hierarchy:\r\n",
        "[[[[0, 11, 19, 27, 35],\r\n",
        "   [45],\r\n",
        "   [37],\r\n",
        "   [9],\r\n",
        "   [26],\r\n",
        "   [28],\r\n",
        "   [56],\r\n",
        "   [15],\r\n",
        "   [47],\r\n",
        "   [54],\r\n",
        "   [20],\r\n",
        "   [5],\r\n",
        "   [10],\r\n",
        "   [33],\r\n",
        "   [59],\r\n",
        "   [4]],\r\n",
        "  [3, 12, 13, 23, 29, 30, 38, 42, 51, 57, 62],\r\n",
        "  [32, 50],\r\n",
        "  [24, 49],\r\n",
        "  [8],\r\n",
        "  [14],\r\n",
        "  [63],\r\n",
        "  [48],\r\n",
        "  [58],\r\n",
        "  [43],\r\n",
        "  [53],\r\n",
        "  [60],\r\n",
        "  [40],\r\n",
        "  [34],\r\n",
        "  [6],\r\n",
        "  [17]],\r\n",
        " [2, 7, 46],\r\n",
        " [55],\r\n",
        " [41],\r\n",
        " [18],\r\n",
        " [31],\r\n",
        " [39],\r\n",
        " [21],\r\n",
        " [16],\r\n",
        " [1],\r\n",
        " [25],\r\n",
        " [22],\r\n",
        " [36],\r\n",
        " [52],\r\n",
        " [61],\r\n",
        " [44]]\r\n",
        " done\r\n",
        "Loading model file data/topicmodelling/kiva.lda_model ... done\r\n",
        "Building nested hierarchy in memory ... done\r\n",
        " Dumping object hierarchy into JSON file data/topicmodelling/kivadata.json ... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done\r\n"
       ]
      }
     ],
     "prompt_number": 172
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO\n",
      "\n",
      "* Visualize with [Zoomable Sunburst](http://bl.ocks.org/mbostock/4348373), provided it does something useful with the 'size' attribute. May ditch Hierarchie altogether\n",
      "* Example [data](http://bl.ocks.org/mbostock/raw/4063550/flare.json) (without words)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}